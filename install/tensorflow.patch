diff --git a/tensorflow/core/grappler/optimizers/remapper.cc b/tensorflow/core/grappler/optimizers/remapper.cc
index d549cbdc..f07b475d 100644
--- a/tensorflow/core/grappler/optimizers/remapper.cc
+++ b/tensorflow/core/grappler/optimizers/remapper.cc
@@ -913,6 +913,13 @@ Status AddFusedContractionNode(RemapperContext* ctx,
   VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd: "
           << " bias_add=" << bias_add.name()
           << " contraction=" << contraction.name();
+ 
+#if defined(DISABLE_TF_MATMUL_FUSION)
+  if (IsMatMul(contraction)) {
+    VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+    return Status::OK();
+  }
+#endif
 
   NodeDef fused_op;
   fused_op.set_name(bias_add.name());
@@ -952,6 +959,14 @@ Status AddFusedContractionNode(
   const NodeDef& contraction = graph->node(matched.contraction);
   const NodeDef& bias_add = graph->node(matched.bias_add);
   const NodeDef& activation = graph->node(matched.activation);
+ 
+#if defined(DISABLE_TF_MATMUL_FUSION)
+  if (IsMatMul(contraction)) {
+    VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+    return Status::OK();
+  }
+#endif
+
   VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd and "
           << activation.op() << ":"
           << " activation=" << activation.name()

diff --git a/WORKSPACE b/WORKSPACE
index ad645add..983dd9b0 100644
--- a/WORKSPACE
+++ b/WORKSPACE
@@ -24,6 +24,10 @@ load("@io_bazel_rules_closure//closure:defs.bzl", "closure_repositories")
 
 closure_repositories()
 
+load("//third_party/openblas:repo.bzl", "load_openblas")
+
+load_openblas()
+
 load("//third_party/toolchains/preconfig/generate:archives.bzl",
      "bazel_toolchains_archive")
 
diff --git a/tensorflow/compiler/xla/service/cpu/BUILD b/tensorflow/compiler/xla/service/cpu/BUILD
index 7e1b8a1e..6e4988c4 100644
--- a/tensorflow/compiler/xla/service/cpu/BUILD
+++ b/tensorflow/compiler/xla/service/cpu/BUILD
@@ -116,6 +116,7 @@ cc_library(
         ":target_machine_features",
         "@com_google_absl//absl/base",
         "@com_google_absl//absl/types:span",
+        "//tensorflow/compiler/xla/ANNC/annc/service/cpu:annc",
         "//tensorflow/compiler/xla/service:copy_insertion",
         "//tensorflow/compiler/xla/service:hlo_casting_utils",
         "//tensorflow/compiler/xla/service:dump",
@@ -175,6 +176,10 @@ cc_library(
         "//tensorflow/compiler/xla/service/llvm_ir:llvm_util",
         "//tensorflow/core:lib",
         "//tensorflow/core:stream_executor_no_cuda",
+        # annc
+        "//tensorflow/compiler/xla/service:custom_call_target_registry",
+        "//tensorflow/compiler/xla/service:pattern_matcher",
+        # annc
         "@llvm-project//llvm:core",
         "@llvm-project//llvm:mc",
         "@llvm-project//llvm:object",
ddiff --git a/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc b/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
index 8fbe29f4..f4a4938b 100644
--- a/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
+++ b/tensorflow/compiler/xla/service/cpu/cpu_compiler.cc
@@ -113,6 +113,7 @@ limitations under the License.
 #include "tensorflow/compiler/xla/util.h"
 #include "tensorflow/compiler/xla/xla_data.pb.h"
 #include "tensorflow/core/platform/dynamic_annotations.h"
+#include "tensorflow/compiler/xla/ANNC/annc/service/cpu/kdnn_rewriter.h"
 
 namespace xla {
 namespace cpu {
@@ -233,6 +234,8 @@ class CollectProfileCandidates : public DfsHloVisitorWithDefault {
 
 }  // namespace
 
+REGISTER_ALL_GEMM_KERNELS()
+
 Status CpuCompiler::RunHloPassesThroughLayoutAssn(
     HloModule* module, bool /*is_aot_compile*/,
     LLVMTargetMachineFeatures* target_machine_features) {
@@ -330,6 +333,7 @@ Status CpuCompiler::RunHloPassesThroughLayoutAssn(
       LayoutAssignment::InstructionCanChangeLayout, target_machine_features);
 
   pipeline.AddPass<CpuInstructionFusion>();
+  pipeline.AddPass<KDnnFusionAfterHloLayoutAssign>();
 
   return pipeline.Run(module).status();
 }
