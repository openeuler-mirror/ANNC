diff --git a/tensorflow/core/grappler/optimizers/BUILD b/tensorflow/core/grappler/optimizers/BUILD
index ecd55973..faafa651 100644
--- a/tensorflow/core/grappler/optimizers/BUILD
+++ b/tensorflow/core/grappler/optimizers/BUILD
@@ -880,12 +880,34 @@ tf_cuda_cc_test(
     ],
 )
 
+alias(
+    name = "is_aarch64",
+    actual = "@local_xla//xla/service/cpu:is_aarch64",
+    visibility = ["//visibility:public"],
+)
+
+alias(
+    name = "aarch64_and_annc_disabled",
+    actual = "@local_xla//xla/service/cpu:aarch64_and_annc_disabled",
+    visibility = ["//visibility:public"],
+)
+
 tf_kernel_library(
     name = "remapper",
     srcs = ["remapper.cc"],
     hdrs = [
         "remapper.h",
     ],
+    copts = select({
+        ":aarch64_and_annc_disabled": [],
+        ":is_aarch64": ["-DENABLE_ANNC"],
+        "//conditions:default": [],
+    }),
+    linkopts = select({
+        ":aarch64_and_annc_disabled": [],
+        ":is_aarch64": ["-lannc"],
+        "//conditions:default": [],
+    }),
     visibility = ["//visibility:public"],
     deps = [
         ":constant_folding",
diff --git a/tensorflow/core/grappler/optimizers/remapper.cc b/tensorflow/core/grappler/optimizers/remapper.cc
index 3c37150f..aad89cd8 100644
--- a/tensorflow/core/grappler/optimizers/remapper.cc
+++ b/tensorflow/core/grappler/optimizers/remapper.cc
@@ -52,6 +52,11 @@ limitations under the License.
 #include "third_party/gpus/cudnn/cudnn.h"
 #endif  // GOOGLE_CUDA
 
+#ifdef ENABLE_ANNC
+#include <annc/annc_flags.h>
+#endif
+bool disable_tf_matmul_fusion = false;
+
 namespace tensorflow {
 namespace grappler {
 
@@ -3102,6 +3107,14 @@ Status AddFusedContractionNode(RemapperContext* ctx,
           << " bias_add=" << bias_add.name()
           << " contraction=" << contraction.name();
 
+  if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
+
   NodeDef fused_op;
   fused_op.set_name(bias_add.name());
   fused_op.set_device(contraction.device());
@@ -3200,6 +3213,13 @@ Status AddFusedContractionNode(
   const NodeDef& bias_add = graph->node(matched.bias_add);
   const NodeDef& activation = graph->node(matched.activation);
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd and "
           << activation.op() << ":"
           << " activation=" << activation.name()
@@ -3396,6 +3416,13 @@ Status AddFusedContractionNode(RemapperContext* ctx,
   DCHECK(IsConv2D(contraction) || IsMatMul(contraction) ||
          IsConv3D(contraction));
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   NodeDef contraction_node;
   const NodeDef& add = graph->node(matched.add);
   contraction_node.set_name(add.name());
@@ -3600,6 +3627,13 @@ Status AddFusedMatMulBiasAddAndGelu(
   auto* matmul_node =
       ctx->graph_view.GetNode(matched_nodes_map.at("matmul"))->node();
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(*matmul_node)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   NodeDef fused_node;
   // Fused node should have the name of terminal node of the fusion.
   fused_node.set_name(output_node->name());
@@ -4595,6 +4629,10 @@ bool RequiresInferredShapes(const RemapperContext& ctx, int node_index,
 
 Status Remapper::Optimize(Cluster* cluster, const GrapplerItem& item,
                           GraphDef* optimized_graph) {
+#ifdef ENABLE_ANNC
+  auto& flags = annc::get_annc_flags();
+  if (flags.is_enabled("disable-tf-matmul-fusion")) disable_tf_matmul_fusion = true;
+#endif
   GrapplerItem mutable_item = item;
   Status status;
   RemapperContext ctx(&mutable_item, &status, cpu_layout_conversion_,
diff --git a/third_party/xla/xla/service/cpu/BUILD b/third_party/xla/xla/service/cpu/BUILD
index 6e0ea613..be5a9d57 100644
--- a/third_party/xla/xla/service/cpu/BUILD
+++ b/third_party/xla/xla/service/cpu/BUILD
@@ -198,11 +198,37 @@ cc_library(
     ],
 )
 
+config_setting(
+    name = "disable_annc",
+    define_values = {"disable_annc": "false"},
+    visibility = ["//visibility:public"],
+)
+
+config_setting(
+    name = "is_aarch64",
+    constraint_values = ["@platforms//cpu:aarch64"],
+)
+
+config_setting(
+    name = "aarch64_and_annc_disabled",
+    constraint_values = ["@platforms//cpu:aarch64"],
+    define_values = {"disable_annc": "true"},
+)
+
 cc_library(
     name = "cpu_compiler_pure",
     srcs = ["cpu_compiler.cc"],
     hdrs = ["cpu_compiler.h"],
-    copts = tsl_copts(),
+    copts = tsl_copts() + select({
+        ":aarch64_and_annc_disabled": [],
+        ":is_aarch64": ["-DENABLE_ANNC"],
+        "//conditions:default": [],
+    }),
+    linkopts = select({
+        ":aarch64_and_annc_disabled": [],
+        ":is_aarch64": ["-lannc", "-fopenmp"],
+        "//conditions:default": [],
+    }),
     visibility = ["//visibility:public"],
     deps = [
         ":buffer_info_util",
diff --git a/third_party/xla/xla/service/cpu/cpu_compiler.cc b/third_party/xla/xla/service/cpu/cpu_compiler.cc
index e519cf05..811f873b 100644
--- a/third_party/xla/xla/service/cpu/cpu_compiler.cc
+++ b/third_party/xla/xla/service/cpu/cpu_compiler.cc
@@ -238,6 +238,11 @@ limitations under the License.
 #include "xla/service/cpu/onednn_rewriter.h"
 #endif
 
+#ifdef ENABLE_ANNC
+#include <annc/kdnn_rewriter.h>
+#include <annc/annc_flags.h>
+#endif
+
 namespace {
 
 // We need to explicitly load all the dialects we will involved in emitting the
@@ -617,6 +622,20 @@ void AddHloVerifier(HloPassPipeline* pipeline, bool allow_sparse_shapes,
 
 }  // namespace
 
+namespace {
+    const bool kAnncKernelRegistered = []() {
+#ifdef ENABLE_ANNC
+        auto& flags = annc::get_annc_flags();
+        if (flags.is_enabled("annc-pass")) {
+          REGISTER_ALL_GEMM_KERNELS()
+          VLOG(1) << "#### register all gemm kernels ";
+        }
+#endif
+        return true;
+    } ();
+}
+
+
 Status CpuCompiler::RunHloPassesThroughLayoutAssn(
     HloModule* module, bool is_aot_compile,
     LLVMTargetMachineFeatures* target_machine_features, bool is_mlir_compile) {
@@ -844,6 +863,15 @@ Status CpuCompiler::RunHloPassesThroughLayoutAssn(
         &layout_constraints);
   }
 
+
+#ifdef ENABLE_ANNC
+  auto& flags = annc::get_annc_flags();
+  if (flags.is_enabled("annc-pass")) {
+    VLOG(1) << "#### Run KDnnFusion Pass" ;
+    pipeline.AddPass<KDnnFusionAfterHloLayoutAssign>();
+  }
+#endif
+
   return pipeline.Run(module).status();
 }
 

