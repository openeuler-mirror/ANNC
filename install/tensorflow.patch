diff --git a/tensorflow/core/grappler/optimizers/BUILD b/tensorflow/core/grappler/optimizers/BUILD
index ecd55973..76cd55d7 100644
--- a/tensorflow/core/grappler/optimizers/BUILD
+++ b/tensorflow/core/grappler/optimizers/BUILD
@@ -886,6 +886,7 @@ tf_kernel_library(
     hdrs = [
         "remapper.h",
     ],
+    linkopts = ["-lannc"],
     visibility = ["//visibility:public"],
     deps = [
         ":constant_folding",
diff --git a/tensorflow/core/grappler/optimizers/remapper.cc b/tensorflow/core/grappler/optimizers/remapper.cc
index 3c37150f..09d65e52 100644
--- a/tensorflow/core/grappler/optimizers/remapper.cc
+++ b/tensorflow/core/grappler/optimizers/remapper.cc
@@ -52,6 +52,9 @@ limitations under the License.
 #include "third_party/gpus/cudnn/cudnn.h"
 #endif  // GOOGLE_CUDA
 
+#include <annc/annc_flags.h>
+bool disable_tf_matmul_fusion = false;
+
 namespace tensorflow {
 namespace grappler {
 
@@ -3102,6 +3105,14 @@ Status AddFusedContractionNode(RemapperContext* ctx,
           << " bias_add=" << bias_add.name()
           << " contraction=" << contraction.name();
 
+  if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
+
   NodeDef fused_op;
   fused_op.set_name(bias_add.name());
   fused_op.set_device(contraction.device());
@@ -3200,6 +3211,13 @@ Status AddFusedContractionNode(
   const NodeDef& bias_add = graph->node(matched.bias_add);
   const NodeDef& activation = graph->node(matched.activation);
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   VLOG(2) << "Fuse " << contraction.op() << " with BiasAdd and "
           << activation.op() << ":"
           << " activation=" << activation.name()
@@ -3396,6 +3414,13 @@ Status AddFusedContractionNode(RemapperContext* ctx,
   DCHECK(IsConv2D(contraction) || IsMatMul(contraction) ||
          IsConv3D(contraction));
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(contraction)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   NodeDef contraction_node;
   const NodeDef& add = graph->node(matched.add);
   contraction_node.set_name(add.name());
@@ -3600,6 +3625,13 @@ Status AddFusedMatMulBiasAddAndGelu(
   auto* matmul_node =
       ctx->graph_view.GetNode(matched_nodes_map.at("matmul"))->node();
 
+ if (disable_tf_matmul_fusion) {
+    if (IsMatMul(*matmul_node)) {
+      VLOG(1) << " Disable MatMul operator fusion in TensorFlow. ";
+      return OkStatus();
+    }
+  }
+
   NodeDef fused_node;
   // Fused node should have the name of terminal node of the fusion.
   fused_node.set_name(output_node->name());
@@ -4595,6 +4627,8 @@ bool RequiresInferredShapes(const RemapperContext& ctx, int node_index,
 
 Status Remapper::Optimize(Cluster* cluster, const GrapplerItem& item,
                           GraphDef* optimized_graph) {
+  auto& flags = annc::get_annc_flags();
+  if (flags.is_enabled("disable-tf-matmul-fusion")) disable_tf_matmul_fusion = true;
   GrapplerItem mutable_item = item;
   Status status;
   RemapperContext ctx(&mutable_item, &status, cpu_layout_conversion_,
diff --git a/third_party/xla/xla/service/cpu/BUILD b/third_party/xla/xla/service/cpu/BUILD
index 6e0ea613..158bb0f7 100644
--- a/third_party/xla/xla/service/cpu/BUILD
+++ b/third_party/xla/xla/service/cpu/BUILD
@@ -203,6 +203,7 @@ cc_library(
     srcs = ["cpu_compiler.cc"],
     hdrs = ["cpu_compiler.h"],
     copts = tsl_copts(),
+    linkopts = ["-L/usr/lib64", "-lannc", "-fopenmp"],
     visibility = ["//visibility:public"],
     deps = [
         ":buffer_info_util",
