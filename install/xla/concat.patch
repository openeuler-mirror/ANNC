diff --git a/xla/debug_options_flags.cc b/xla/debug_options_flags.cc
index 31722bfee3..9f595d74d8 100644
--- a/xla/debug_options_flags.cc
+++ b/xla/debug_options_flags.cc
@@ -161,6 +161,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {
       std::numeric_limits<int64_t>::max());
 
   opts.set_xla_cpu_enable_mlir_tiling_and_fusion(true);
+  opts.set_xla_cpu_enable_concat_optimization(false);
   opts.set_xla_cpu_enable_custom_matmul_tiling(false);
   opts.set_xla_cpu_matmul_tiling_m_dim(8);
   opts.set_xla_cpu_matmul_tiling_n_dim(8);
@@ -1067,6 +1068,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_mlir_tiling_and_fusion),
       debug_options->xla_cpu_enable_mlir_tiling_and_fusion(),
       "Enable MLIR tiling and fusion."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_enable_concat_optimization",
+      bool_setter_for(&DebugOptions::set_xla_cpu_enable_concat_optimization),
+      debug_options->xla_cpu_enable_concat_optimization(),
+      "Enable concat optimization."));
   flag_list->push_back(tsl::Flag(
       "xla_cpu_enable_mlir_fusion_outlining",
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_mlir_fusion_outlining),
diff --git a/xla/mlir_hlo/BUILD b/xla/mlir_hlo/BUILD
index f55e0dad52..adcccbb0f5 100644
--- a/xla/mlir_hlo/BUILD
+++ b/xla/mlir_hlo/BUILD
@@ -1292,6 +1292,8 @@ cc_library(
         "transforms/propagate_static_shapes_to_kernel.cc",
         "transforms/test_hlo_transform_dialect_interpreter.cc",
         "transforms/tile_loops_pass.cc",
+        "transforms/tile_linalg_copy_pass.cc",
+        "transforms/linalg_copy_to_memref_pass.cc",
         "transforms/unbufferize_pass.cc",
         "transforms/unroll_loops.cc",
     ],
@@ -1994,6 +1996,7 @@ cc_library(
     name = "thlo_passes",
     srcs = [
         "thlo/transforms/legalize_sort/legalize_sort.cc",
+        "thlo/transforms/legalize_concat/legalize_concat.cc",
         "thlo/transforms/thlo_passes.h.inc",
     ],
     hdrs = [
diff --git a/xla/mlir_hlo/tests/Dialect/thlo/concat.mlir b/xla/mlir_hlo/tests/Dialect/thlo/concat.mlir
new file mode 100644
index 0000000000..807e8994c7
--- /dev/null
+++ b/xla/mlir_hlo/tests/Dialect/thlo/concat.mlir
@@ -0,0 +1,100 @@
+// RUN: mlir-hlo-opt --thlo-legalize-concat %s
+
+// Step 1: Lower thlo.concatenate into tensor.concat
+func.func @step1(%arg0: tensor<128x32xf32>, %arg1: tensor<128x32xf32>) -> tensor<128x64xf32> {
+  %0 = tensor.empty() : tensor<128x64xf32>
+  %concat = tensor.concat dim(1) %arg0, %arg1 : (tensor<128x32xf32>, tensor<128x32xf32>) -> tensor<128x64xf32>
+  return %concat : tensor<128x64xf32>
+}
+
+// RUN: mlir-opt --transform-interpreter %s
+
+// Step 2: Concat decomposition (this is an upstream op)
+func.func @step2(%arg0: tensor<128x32xf32>, %arg1: tensor<128x32xf32>) -> tensor<128x64xf32> {
+  %0 = tensor.empty() : tensor<128x64xf32>
+  %concat = tensor.concat dim(1) %arg0, %arg1 : (tensor<128x32xf32>, tensor<128x32xf32>) -> tensor<128x64xf32>
+  return %concat : tensor<128x64xf32>
+}
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @__transform_main(%root: !transform.any_op {transform.readonly}) {
+    %func_op = transform.structured.match ops{["func.func"]} in %root : (!transform.any_op) -> !transform.op<"func.func">
+    transform.apply_patterns to %func_op {
+      transform.apply_patterns.tensor.decompose_concat
+    } : !transform.op<"func.func">
+    transform.yield
+  }
+}
+
+// Step 3: Bufferize (with memcpy_op=linalg.copy)
+func.func @step3(%arg0: tensor<128x32xf32>, %arg1: tensor<128x32xf32>) -> tensor<128x64xf32> {
+  %0 = tensor.empty() : tensor<128x64xf32>
+  %inserted_slice = tensor.insert_slice %arg0 into %0[0, 0] [128, 32] [1, 1] : tensor<128x32xf32> into tensor<128x64xf32>
+  %inserted_slice_0 = tensor.insert_slice %arg1 into %inserted_slice[0, 32] [128, 32] [1, 1] : tensor<128x32xf32> into tensor<128x64xf32>
+  return %inserted_slice_0 : tensor<128x64xf32>
+}
+module attributes {transform.with_named_sequence} {
+  transform.named_sequence @__transform_main(%arg0: !transform.any_op {transform.consumed}) {
+    %0 = transform.structured.match ops{["tensor.empty"]} in %arg0 : (!transform.any_op) -> !transform.any_op
+    %1 = transform.cast %0 : !transform.any_op to !transform.op<"tensor.empty">
+    %2 = transform.bufferization.empty_tensor_to_alloc_tensor %1 : (!transform.op<"tensor.empty">) -> !transform.op<"bufferization.alloc_tensor">
+    %3 = transform.bufferization.one_shot_bufferize %arg0 {bufferize_function_boundaries = true, function_boundary_type_conversion = 1 : i32, memcpy_op = "linalg.copy"} : (!transform.any_op) -> !transform.any_op
+    transform.yield
+  }
+}
+
+// RUN: mlir-hlo-opt --tile-linalg-copy --canonicalize %s
+
+// Step 4: Tile the linalg copy with tile size of 1
+func.func @step4(%arg0: memref<128x32xf32>, %arg1: memref<128x32xf32>) -> memref<128x64xf32> {
+  %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64xf32>
+  %subview = memref.subview %alloc[0, 0] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1]>>
+  linalg.copy ins(%arg0 : memref<128x32xf32>) outs(%subview : memref<128x32xf32, strided<[64, 1]>>)
+  %subview_0 = memref.subview %alloc[0, 32] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1], offset: 32>>
+  linalg.copy ins(%arg1 : memref<128x32xf32>) outs(%subview_0 : memref<128x32xf32, strided<[64, 1], offset: 32>>)
+  return %alloc : memref<128x64xf32>
+}
+
+// RUN: mlir-hlo-opt --tile-linalg-copy --canonicalize %s
+
+// Step 5: Replace the linalg.copy ops with memref.copy
+func.func @step5(%arg0: memref<128x32xf32>, %arg1: memref<128x32xf32>) -> memref<128x64xf32> {
+  %c128 = arith.constant 128 : index
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64xf32>
+  %subview = memref.subview %alloc[0, 0] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1]>>
+  scf.for %arg2 = %c0 to %c128 step %c1 {
+    %subview_1 = memref.subview %arg0[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32> to memref<1x32xf32, strided<[32, 1], offset: ?>>
+    %subview_2 = memref.subview %subview[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32, strided<[64, 1]>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+    linalg.copy ins(%subview_1 : memref<1x32xf32, strided<[32, 1], offset: ?>>) outs(%subview_2 : memref<1x32xf32, strided<[64, 1], offset: ?>>)
+  }
+  %subview_0 = memref.subview %alloc[0, 32] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1], offset: 32>>
+  scf.for %arg2 = %c0 to %c128 step %c1 {
+    %subview_1 = memref.subview %arg1[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32> to memref<1x32xf32, strided<[32, 1], offset: ?>>
+    %subview_2 = memref.subview %subview_0[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32, strided<[64, 1], offset: 32>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+    linalg.copy ins(%subview_1 : memref<1x32xf32, strided<[32, 1], offset: ?>>) outs(%subview_2 : memref<1x32xf32, strided<[64, 1], offset: ?>>)
+  }
+  return %alloc : memref<128x64xf32>
+}
+
+// Final output
+func.func @result(%arg0: memref<128x32xf32>, %arg1: memref<128x32xf32>) -> memref<128x64xf32> {
+  %c128 = arith.constant 128 : index
+  %c0 = arith.constant 0 : index
+  %c1 = arith.constant 1 : index
+  %alloc = memref.alloc() {alignment = 64 : i64} : memref<128x64xf32>
+  %subview = memref.subview %alloc[0, 0] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1]>>
+  scf.for %arg2 = %c0 to %c128 step %c1 {
+    %subview_1 = memref.subview %arg0[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32> to memref<1x32xf32, strided<[32, 1], offset: ?>>
+    %subview_2 = memref.subview %subview[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32, strided<[64, 1]>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+    memref.copy %subview_1, %subview_2 : memref<1x32xf32, strided<[32, 1], offset: ?>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+  }
+  %subview_0 = memref.subview %alloc[0, 32] [128, 32] [1, 1] : memref<128x64xf32> to memref<128x32xf32, strided<[64, 1], offset: 32>>
+  scf.for %arg2 = %c0 to %c128 step %c1 {
+    %subview_1 = memref.subview %arg1[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32> to memref<1x32xf32, strided<[32, 1], offset: ?>>
+    %subview_2 = memref.subview %subview_0[%arg2, 0] [1, 32] [1, 1] : memref<128x32xf32, strided<[64, 1], offset: 32>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+    memref.copy %subview_1, %subview_2 : memref<1x32xf32, strided<[32, 1], offset: ?>> to memref<1x32xf32, strided<[64, 1], offset: ?>>
+  }
+  return %alloc : memref<128x64xf32>
+}
+
diff --git a/xla/mlir_hlo/thlo/transforms/CMakeLists.txt b/xla/mlir_hlo/thlo/transforms/CMakeLists.txt
index d582d86a72..daae82e9aa 100644
--- a/xla/mlir_hlo/thlo/transforms/CMakeLists.txt
+++ b/xla/mlir_hlo/thlo/transforms/CMakeLists.txt
@@ -23,6 +23,7 @@ include_directories(BEFORE
 
 add_mlir_library(ThloPasses
   legalize_sort/legalize_sort.cc
+  legalize_sort/legalize_concat.cc
 
   DEPENDS
   MLIRThloPassIncGen
@@ -34,5 +35,6 @@ add_mlir_library(ThloPasses
   MLIRMemRefDialect
   MLIRPass
   MLIRSCFDialect
+  MLIRTensorDialect
   MLIRTransforms
 )
diff --git a/xla/mlir_hlo/thlo/transforms/legalize_concat/legalize_concat.cc b/xla/mlir_hlo/thlo/transforms/legalize_concat/legalize_concat.cc
new file mode 100644
index 0000000000..41474a3571
--- /dev/null
+++ b/xla/mlir_hlo/thlo/transforms/legalize_concat/legalize_concat.cc
@@ -0,0 +1,69 @@
+#include <iterator>
+#include <memory>
+#include <optional>
+#include <utility>
+
+
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+#include "thlo/IR/thlo_ops.h"
+#include "thlo/transforms/passes.h"
+
+namespace mlir {
+namespace thlo {
+
+#define GEN_PASS_DEF_THLOLEGALIZECONCATPASS
+#include "thlo/transforms/thlo_passes.h.inc"
+
+namespace {
+
+struct ConcatenateOpPattern : public OpRewritePattern<ConcatenateOp> {
+  using OpRewritePattern::OpRewritePattern;
+
+  LogicalResult matchAndRewrite(ConcatenateOp op,
+                                PatternRewriter& rewriter) const override {    
+    // Rewrite must happen before bufferization.
+    if (op.hasBufferSemantics())
+      return op->emitError() << "expected tensor semantics";
+    
+    int64_t concatDim = op.getDimension().getSExtValue();
+
+    rewriter.replaceOpWithNewOp<tensor::ConcatOp>(op, /*dim=*/ concatDim,
+                                                    /*inputs=*/ op.getInputs());
+    return success();
+  }
+};  
+
+struct LegalizeConcatPass
+    : public impl::ThloLegalizeConcatPassBase<LegalizeConcatPass> {
+  // Rewrites a thlo.concatenate as a tensor.concat with identical operands and
+  // semantics.
+  void runOnOperation() override {
+    func::FuncOp f = getOperation();
+    MLIRContext* ctx = f.getContext();
+	
+    RewritePatternSet patterns(ctx);
+    patterns.add<ConcatenateOpPattern>(ctx);
+
+    mlir::ConversionTarget target(*ctx);
+    target.markUnknownOpDynamicallyLegal([](Operation*) { return true; });
+    target.addIllegalOp<thlo::ConcatenateOp>();
+
+    if (failed(applyPartialConversion(f, target, std::move(patterns)))) {
+      signalPassFailure();
+    }
+  }
+};
+
+}  // namespace
+
+}  // namespace thlo
+}  // namespace mlir
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
+mlir::thlo::createLegalizeConcatPass() {
+  return std::make_unique<LegalizeConcatPass>();
+}
diff --git a/xla/mlir_hlo/thlo/transforms/passes.h b/xla/mlir_hlo/thlo/transforms/passes.h
index 7ac8499f71..1a51f9e9da 100644
--- a/xla/mlir_hlo/thlo/transforms/passes.h
+++ b/xla/mlir_hlo/thlo/transforms/passes.h
@@ -32,11 +32,15 @@ class FuncOp;
 namespace thlo {
 
 #define GEN_PASS_DECL_THLOLEGALIZESORTPASS
+#define GEN_PASS_DECL_THLOLEGALIZECONCATPASS
 #include "thlo/transforms/thlo_passes.h.inc"
 
 /// Lowers sort to Arith, MemRef, and SCF
 std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeSortPass();
 
+/// Lowers sort to Vector
+std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeConcatPass();
+
 #define GEN_PASS_REGISTRATION
 #include "thlo/transforms/thlo_passes.h.inc"
 
diff --git a/xla/mlir_hlo/thlo/transforms/thlo_passes.td b/xla/mlir_hlo/thlo/transforms/thlo_passes.td
index be0bdf4381..ade70b3765 100644
--- a/xla/mlir_hlo/thlo/transforms/thlo_passes.td
+++ b/xla/mlir_hlo/thlo/transforms/thlo_passes.td
@@ -21,4 +21,11 @@ def ThloLegalizeSortPass : Pass<"thlo-legalize-sort", "func::FuncOp"> {
   let constructor = "createLegalizeSortPass()";
   let dependentDialects = ["arith::ArithDialect", "memref::MemRefDialect",
                            "scf::SCFDialect"];
+}
+
+def ThloLegalizeConcatPass : Pass<"thlo-legalize-concat", "func::FuncOp"> {
+  let summary =
+    "Legalize from THLO concat on tensors to MLIR upstream tensor.concat.";
+  let constructor = "createLegalizeConcatPass()";
+  let dependentDialects = ["tensor::TensorDialect"];
 }
\ No newline at end of file
diff --git a/xla/mlir_hlo/transforms/CMakeLists.txt b/xla/mlir_hlo/transforms/CMakeLists.txt
index 4d61e07d8e..02b2e7e4b6 100644
--- a/xla/mlir_hlo/transforms/CMakeLists.txt
+++ b/xla/mlir_hlo/transforms/CMakeLists.txt
@@ -34,6 +34,8 @@ add_mlir_library(MLIRBufferTransforms
   propagate_static_shapes_to_kernel.cc
   test_hlo_transform_dialect_interpreter.cc
   tile_loops_pass.cc
+  tile_linalg_copy_pass.cc
+  linalg_copy_to_memref_pass.cc
   unbufferize_pass.cc
   unroll_loops.cc
 
diff --git a/xla/mlir_hlo/transforms/bufferize_pass.cc b/xla/mlir_hlo/transforms/bufferize_pass.cc
index 9c2d3e4b9c..07c5d5b22c 100644
--- a/xla/mlir_hlo/transforms/bufferize_pass.cc
+++ b/xla/mlir_hlo/transforms/bufferize_pass.cc
@@ -217,6 +217,14 @@ struct ComputeOpAndFuncBufferizePass
 
 struct OneShotBufferizePass
     : public impl::OneShotBufferizeBase<OneShotBufferizePass> {
+  private:
+    bool useLinalgCopyFn;
+
+  public:
+    OneShotBufferizePass(bool useLinalgCopyFn_) {
+      useLinalgCopyFn = useLinalgCopyFn_;
+    }
+
   // TODO(b/173201243): Move to tablegen.
   void getDependentDialects(DialectRegistry& registry) const override {
     registry.insert<bufferization::BufferizationDialect, lmhlo::LmhloDialect,
@@ -255,6 +263,16 @@ struct OneShotBufferizePass
     opts.inferFunctionResultLayout = false;
     opts.bufferAlignment = 64;
 
+    if (useLinalgCopyFn) {
+      opts.setFunctionBoundaryTypeConversion(
+          bufferization::LayoutMapOption::IdentityLayoutMap);
+      // Equivalent to memcpy_op=linalg.copy
+      opts.memCpyFn = [](OpBuilder &b, Location loc, Value from, Value to) {
+        b.create<linalg::CopyOp>(loc, from, to);
+        return success();
+      };
+    }
+
     ModuleOp module = getOperation();
     if (failed(bufferization::runOneShotModuleBufferize(module, opts))) {
       signalPassFailure();
@@ -359,8 +377,8 @@ struct FinalBufferizePass
 }  // namespace
 
 namespace hlo {
-std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass() {
-  return std::make_unique<OneShotBufferizePass>();
+std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass(bool useLinalgCopyFn) {
+  return std::make_unique<OneShotBufferizePass>(useLinalgCopyFn);
 }
 }  // namespace hlo
 
diff --git a/xla/mlir_hlo/transforms/linalg_copy_to_memref_pass.cc b/xla/mlir_hlo/transforms/linalg_copy_to_memref_pass.cc
new file mode 100644
index 0000000000..91132759a7
--- /dev/null
+++ b/xla/mlir_hlo/transforms/linalg_copy_to_memref_pass.cc
@@ -0,0 +1,73 @@
+#include <cstdint>
+
+#include "mlir/Dialect/Linalg/IR/Linalg.h"
+#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
+#include "transforms/passes.h"
+
+#include "mlir/Dialect/Arith/Utils/Utils.h"
+#include "mlir/Dialect/Func/IR/FuncOps.h"
+#include "mlir/Dialect/Tensor/IR/Tensor.h"
+#include "mlir/Pass/Pass.h"
+#include "mlir/Transforms/DialectConversion.h"
+
+
+namespace mlir {
+
+#define GEN_PASS_DEF_LINALGCOPYTOMEMREFPASS
+#include "transforms/passes.h.inc"
+
+namespace {
+
+struct LinalgToMemrefCopyPattern : public OpRewritePattern<linalg::CopyOp> {
+  using OpRewritePattern::OpRewritePattern;
+ 
+  LogicalResult matchAndRewrite(linalg::CopyOp op,
+                                PatternRewriter& rewriter) const override {    
+    // Rewrite must happens after bufferization.        
+    if (!op.hasBufferSemantics()) {
+	    return op->emitError() <<
+        "linalg.copy on tensors cannot be converted into memref.copy";
+	  }
+	
+    // Target can be converted, do it
+    SmallVector<Value> inputs = op.getInputs();
+    SmallVector<Value> outputs = op.getOutputs();
+    assert(inputs.size() == 1 && "expected CopyOp with one input");
+    assert(outputs.size() == 1 && "expected CopyOp with one output");
+ 
+    rewriter.replaceOpWithNewOp<memref::CopyOp>(
+      op, inputs.front(), outputs.front());        
+
+    return success();
+  }
+};  
+
+struct LinalgCopyToMemrefPass
+    : public impl::LinalgCopyToMemrefPassBase<LinalgCopyToMemrefPass> {
+
+  void runOnOperation() override {
+    func::FuncOp f = getOperation();
+    MLIRContext* ctx = f.getContext();
+
+    RewritePatternSet patterns(ctx);
+    patterns.add<LinalgToMemrefCopyPattern>(ctx);
+
+    mlir::ConversionTarget target(*ctx);
+    target.markUnknownOpDynamicallyLegal([](Operation*) { return true; });
+    target.addIllegalOp<linalg::CopyOp>();
+
+    if (failed(applyPartialConversion(f, target, std::move(patterns)))) {
+      signalPassFailure();
+    }  
+  }
+};
+
+}  // namespace
+
+} // namespace mlir
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
+mlir::hlo::createLinalgCopyToMemrefPass() {
+  return std::make_unique<LinalgCopyToMemrefPass>();
+}
+
diff --git a/xla/mlir_hlo/transforms/passes.h b/xla/mlir_hlo/transforms/passes.h
index d18c603205..4d89118b55 100644
--- a/xla/mlir_hlo/transforms/passes.h
+++ b/xla/mlir_hlo/transforms/passes.h
@@ -106,7 +106,11 @@ void registerTestHloTransformDialectEraseSchedulePass();
 void registerTestHloTransformDialectInterpreterPass();
 
 namespace hlo {
-std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass();
+std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass(bool useLinalgCopyFn);
+
+std::unique_ptr<Pass> createTileLinalgCopyPass(int tileSize);
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>> createLinalgCopyToMemrefPass();
 
 std::unique_ptr<OperationPass<ModuleOp>> createGenericHostToLLVMPass(
     const GenericHostToLLVMPassOptions& options = {});
diff --git a/xla/mlir_hlo/transforms/passes.td b/xla/mlir_hlo/transforms/passes.td
index b532491a73..9909505055 100644
--- a/xla/mlir_hlo/transforms/passes.td
+++ b/xla/mlir_hlo/transforms/passes.td
@@ -100,7 +100,7 @@ def LowerIndexCastPass
 
 def OneShotBufferize : Pass<"hlo-one-shot-bufferize", "ModuleOp"> {
   let summary = "One shot bufferization pass.";
-  let constructor = "hlo::createOneShotBufferizePass()";
+  let constructor = "hlo::createOneShotBufferizePass(false)";
 }
 
 def ComputeOpAndFuncBufferizePass : Pass<"computeop-and-func-bufferize", "ModuleOp"> {
@@ -158,6 +158,18 @@ def UnbufferizePass : Pass<"unbufferize", "mlir::func::FuncOp"> {
   let constructor = "hlo::createUnbufferizePass()";
 }
 
+def TileLinalgCopyPass : Pass<"tile-linalg-copy"> {
+  let summary = "Tiles linalg copy ops by 1.";
+  let constructor = "hlo::createTileLinalgCopyPass(1)";
+  let dependentDialects = ["linalg::LinalgDialect", "scf::SCFDialect"];
+}
+
+def LinalgCopyToMemrefPass : Pass<"linalg-copy-to-memref", "func::FuncOp"> {
+  let summary = "Rewrites a linalg.copy on memrefs to a memref.copy.";
+  let constructor = "hlo::createLinalgCopyToMemrefPass()";
+  let dependentDialects = ["linalg::LinalgDialect"];
+}
+
 def UnrollLoopsPass : Pass<"unroll-loops"> {
   let summary = "Unrolls scf.for loops with small static iteration counts.";
   let constructor = "hlo::createUnrollLoopsPass()";
diff --git a/xla/mlir_hlo/transforms/tile_linalg_copy_pass.cc b/xla/mlir_hlo/transforms/tile_linalg_copy_pass.cc
new file mode 100644
index 0000000000..6ae0aa1fdc
--- /dev/null
+++ b/xla/mlir_hlo/transforms/tile_linalg_copy_pass.cc
@@ -0,0 +1,66 @@
+#include <cstdint>
+
+#include "mlir/Dialect/Linalg/IR/Linalg.h"
+#include "mlir/Dialect/Linalg/Transforms/Transforms.h"
+#include "transforms/passes.h"
+
+namespace mlir {
+
+#define GEN_PASS_DEF_TILELINALGCOPYPASS
+#include "transforms/passes.h.inc"
+
+namespace {
+
+class TileLinalgCopyPass
+    : public impl::TileLinalgCopyPassBase<TileLinalgCopyPass> {
+public:
+  using TileLinalgCopyPassBase<TileLinalgCopyPass>::TileLinalgCopyPassBase;
+  TileLinalgCopyPass(int tileSize_) {
+    tileSize = tileSize_;
+  }
+
+private:
+  int tileSize;
+  void runOnOperation() override;
+};
+
+} // namespace
+
+void TileLinalgCopyPass::runOnOperation() {
+  MLIRContext *context = &getContext();
+  IRRewriter rewriter(context);
+
+  getOperation()->walk([&](linalg::CopyOp op) {
+    // Tiling must happen after bufferization.
+    if (!op.hasBufferSemantics()) {
+      emitError(op.getLoc(), "expected buffer semantics");
+      signalPassFailure();
+    }
+
+    mlir::linalg::LinalgTilingOptions options;
+    std::vector<int64_t> tile_sizes_vec({tileSize});
+    ArrayRef<int64_t> ts(tile_sizes_vec);
+    options.setTileSizes(ts);
+
+    FailureOr<linalg::TiledLinalgOp> maybeTiled =
+        linalg::tileLinalgOp(rewriter, op, options);
+    if (failed(maybeTiled)) {
+      emitError(op.getLoc(), "failed to tile");
+      signalPassFailure();
+    }
+
+    if (maybeTiled->loops.size() == 1) {
+      // Replace the old linalg.copy with the tiled loop
+      rewriter.replaceOp(op, maybeTiled->loops[0]);
+    } else {
+      emitError(op.getLoc(), "expected to find exactly one loop");
+      signalPassFailure();
+    }
+  });
+}
+
+std::unique_ptr<Pass> hlo::createTileLinalgCopyPass(int tileSize) {
+  return std::make_unique<TileLinalgCopyPass>(tileSize);
+}
+
+} // namespace mlir
diff --git a/xla/service/cpu/cpu_compiler.cc b/xla/service/cpu/cpu_compiler.cc
index d985ca2867..7112ec53a4 100644
--- a/xla/service/cpu/cpu_compiler.cc
+++ b/xla/service/cpu/cpu_compiler.cc
@@ -261,6 +261,11 @@ xla::cpu::HloXlaRuntimePipelineOptions GetHloXlaRuntimePipelineOptions(
   xla::cpu::HloXlaRuntimePipelineOptions options;
   options.enable_tiling_and_fusion =
       xla::GetDebugOptionsFromFlags().xla_cpu_enable_mlir_tiling_and_fusion();
+  options.enable_concat_optimization =
+      xla::GetDebugOptionsFromFlags().xla_cpu_enable_concat_optimization();
+  if (options.enable_concat_optimization) {
+    options.sparse_bufferization = false;
+  }
   if (xla::GetDebugOptionsFromFlags().xla_cpu_enable_custom_matmul_tiling()) {
     options.matmul_tile_sizes = {
         xla::GetDebugOptionsFromFlags().xla_cpu_matmul_tiling_m_dim(),
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.cc b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
index fc2c13fb1c..775f4479e4 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.cc
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
@@ -42,6 +42,7 @@ limitations under the License.
 #include "mlir/Dialect/SparseTensor/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
 #include "mlir/Dialect/SparseTensor/Transforms/Passes.h"  // from @llvm-project
 #include "mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
+#include "mlir/Dialect/Tensor/Transforms/Passes.h" // from @llvm-project
 #include "mlir/Dialect/Vector/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
 #include "mlir/Pass/PassManager.h"  // from @llvm-project
 #include "mlir/Transforms/Passes.h"  // from @llvm-project
@@ -199,7 +200,15 @@ static Status CreateHloXlaPipeline(
 
   // Tile tHLO ops to 1.
   if (!options.enable_tiling_and_fusion) {
-    pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
+    if (options.enable_concat_optimization) {
+      // Instead of tiling by one the thlo.concat (thus generating an
+      // elementwise memory copy), replace it with a tensor.concat.
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::thlo::createLegalizeConcatPass());
+      pm.addPass(mlir::tensor::createDecomposeTensorConcatPass());
+    }
+    else {
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
+    }
   }
 
   // Lower shape dialect to standard to enable linalg canonicalizations (e.g.
@@ -260,7 +269,13 @@ static Status CreateHloXlaPipeline(
     AddSparsificationPasses(pm, options.experimental_deallocation,
                             options.xla_cpu_sparse_cuda_threads);
   } else {
-    pm.addPass(mlir::hlo::createOneShotBufferizePass());
+    pm.addPass(mlir::hlo::createOneShotBufferizePass(options.enable_concat_optimization));
+    if (options.enable_concat_optimization) {
+      // Tiling the linalg copy by 1 makes the linalg.copy work on contiguous
+      // data, which significantly improves copy performance.
+      pm.addPass(mlir::hlo::createTileLinalgCopyPass(1));
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::hlo::createLinalgCopyToMemrefPass());
+    }
   }
   pm.addNestedPass<mlir::func::FuncOp>(createRewriteReallocToAllocPass());
 
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.h b/xla/service/cpu/hlo_xla_runtime_pipeline.h
index b77436c165..64a0160f6d 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.h
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.h
@@ -32,6 +32,7 @@ namespace cpu {
 struct HloXlaRuntimePipelineOptions {
   bool enable_tiling_and_fusion = false;
   bool enable_fusion_outlining = true;
+  bool enable_concat_optimization = true;
   bool remove_copies_to_outparams = true;
   bool sparse_bufferization = true;
   bool experimental_deallocation = false;
diff --git a/xla/xla.proto b/xla/xla.proto
index 7e01954d0c..0e5f8b4276 100644
--- a/xla/xla.proto
+++ b/xla/xla.proto
@@ -531,6 +531,7 @@ message DebugOptions {
   // is enabled, the pipeline will use tiling, fusion, peeling, vectorization
   // instead.
   bool xla_cpu_enable_mlir_tiling_and_fusion = 184;
+  bool xla_cpu_enable_concat_optimization = 258;
 
   // XLA:CPU-Next tiling parameters for matmul.
   bool xla_cpu_enable_custom_matmul_tiling = 195;
