diff --git a/xla/debug_options_flags.cc b/xla/debug_options_flags.cc
index 31722bfee3..9f595d74d8 100644
--- a/xla/debug_options_flags.cc
+++ b/xla/debug_options_flags.cc
@@ -161,6 +161,7 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {
       std::numeric_limits<int64_t>::max());
 
   opts.set_xla_cpu_enable_mlir_tiling_and_fusion(true);
+  opts.set_xla_cpu_enable_concat_optimization(false);
   opts.set_xla_cpu_enable_custom_matmul_tiling(false);
   opts.set_xla_cpu_matmul_tiling_m_dim(8);
   opts.set_xla_cpu_matmul_tiling_n_dim(8);
@@ -1067,6 +1068,11 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_mlir_tiling_and_fusion),
       debug_options->xla_cpu_enable_mlir_tiling_and_fusion(),
       "Enable MLIR tiling and fusion."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_enable_concat_optimization",
+      bool_setter_for(&DebugOptions::set_xla_cpu_enable_concat_optimization),
+      debug_options->xla_cpu_enable_concat_optimization(),
+      "Enable concat optimization."));
   flag_list->push_back(tsl::Flag(
       "xla_cpu_enable_mlir_fusion_outlining",
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_mlir_fusion_outlining),
diff --git a/xla/mlir_hlo/thlo/transforms/passes.h b/xla/mlir_hlo/thlo/transforms/passes.h
index 7ac8499f71..1a51f9e9da 100644
--- a/xla/mlir_hlo/thlo/transforms/passes.h
+++ b/xla/mlir_hlo/thlo/transforms/passes.h
@@ -32,11 +32,15 @@ class FuncOp;
 namespace thlo {
 
 #define GEN_PASS_DECL_THLOLEGALIZESORTPASS
+#define GEN_PASS_DECL_THLOLEGALIZECONCATPASS
 #include "thlo/transforms/thlo_passes.h.inc"
 
 /// Lowers sort to Arith, MemRef, and SCF
 std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeSortPass();
 
+/// Lowers sort to Vector
+std::unique_ptr<OperationPass<func::FuncOp>> createLegalizeConcatPass();
+
 #define GEN_PASS_REGISTRATION
 #include "thlo/transforms/thlo_passes.h.inc"
 
diff --git a/xla/mlir_hlo/thlo/transforms/thlo_passes.td b/xla/mlir_hlo/thlo/transforms/thlo_passes.td
index be0bdf4381..ade70b3765 100644
--- a/xla/mlir_hlo/thlo/transforms/thlo_passes.td
+++ b/xla/mlir_hlo/thlo/transforms/thlo_passes.td
@@ -21,4 +21,11 @@ def ThloLegalizeSortPass : Pass<"thlo-legalize-sort", "func::FuncOp"> {
   let constructor = "createLegalizeSortPass()";
   let dependentDialects = ["arith::ArithDialect", "memref::MemRefDialect",
                            "scf::SCFDialect"];
+}
+
+def ThloLegalizeConcatPass : Pass<"thlo-legalize-concat", "func::FuncOp"> {
+  let summary =
+    "Legalize from THLO concat on tensors to MLIR upstream tensor.concat.";
+  let constructor = "createLegalizeConcatPass()";
+  let dependentDialects = ["tensor::TensorDialect"];
 }
\ No newline at end of file
diff --git a/xla/mlir_hlo/transforms/bufferize_pass.cc b/xla/mlir_hlo/transforms/bufferize_pass.cc
index 9c2d3e4b9c..07c5d5b22c 100644
--- a/xla/mlir_hlo/transforms/bufferize_pass.cc
+++ b/xla/mlir_hlo/transforms/bufferize_pass.cc
@@ -217,6 +217,14 @@ struct ComputeOpAndFuncBufferizePass
 
 struct OneShotBufferizePass
     : public impl::OneShotBufferizeBase<OneShotBufferizePass> {
+  private:
+    bool useLinalgCopyFn;
+
+  public:
+    OneShotBufferizePass(bool useLinalgCopyFn_) {
+      useLinalgCopyFn = useLinalgCopyFn_;
+    }
+
   // TODO(b/173201243): Move to tablegen.
   void getDependentDialects(DialectRegistry& registry) const override {
     registry.insert<bufferization::BufferizationDialect, lmhlo::LmhloDialect,
@@ -255,6 +263,16 @@ struct OneShotBufferizePass
     opts.inferFunctionResultLayout = false;
     opts.bufferAlignment = 64;
 
+    if (useLinalgCopyFn) {
+      opts.setFunctionBoundaryTypeConversion(
+          bufferization::LayoutMapOption::IdentityLayoutMap);
+      // Equivalent to memcpy_op=linalg.copy
+      opts.memCpyFn = [](OpBuilder &b, Location loc, Value from, Value to) {
+        b.create<linalg::CopyOp>(loc, from, to);
+        return success();
+      };
+    }
+
     ModuleOp module = getOperation();
     if (failed(bufferization::runOneShotModuleBufferize(module, opts))) {
       signalPassFailure();
@@ -359,8 +377,8 @@ struct FinalBufferizePass
 }  // namespace
 
 namespace hlo {
-std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass() {
-  return std::make_unique<OneShotBufferizePass>();
+std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass(bool useLinalgCopyFn) {
+  return std::make_unique<OneShotBufferizePass>(useLinalgCopyFn);
 }
 }  // namespace hlo
 
diff --git a/xla/mlir_hlo/transforms/passes.h b/xla/mlir_hlo/transforms/passes.h
index d18c603205..4d89118b55 100644
--- a/xla/mlir_hlo/transforms/passes.h
+++ b/xla/mlir_hlo/transforms/passes.h
@@ -106,7 +106,11 @@ void registerTestHloTransformDialectEraseSchedulePass();
 void registerTestHloTransformDialectInterpreterPass();
 
 namespace hlo {
-std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass();
+std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass(bool useLinalgCopyFn);
+
+std::unique_ptr<Pass> createTileLinalgCopyPass(int tileSize);
+
+std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>> createLinalgCopyToMemrefPass();
 
 std::unique_ptr<OperationPass<ModuleOp>> createGenericHostToLLVMPass(
     const GenericHostToLLVMPassOptions& options = {});
diff --git a/xla/mlir_hlo/transforms/passes.td b/xla/mlir_hlo/transforms/passes.td
index b532491a73..9909505055 100644
--- a/xla/mlir_hlo/transforms/passes.td
+++ b/xla/mlir_hlo/transforms/passes.td
@@ -100,7 +100,7 @@ def LowerIndexCastPass
 
 def OneShotBufferize : Pass<"hlo-one-shot-bufferize", "ModuleOp"> {
   let summary = "One shot bufferization pass.";
-  let constructor = "hlo::createOneShotBufferizePass()";
+  let constructor = "hlo::createOneShotBufferizePass(false)";
 }
 
 def ComputeOpAndFuncBufferizePass : Pass<"computeop-and-func-bufferize", "ModuleOp"> {
@@ -158,6 +158,18 @@ def UnbufferizePass : Pass<"unbufferize", "mlir::func::FuncOp"> {
   let constructor = "hlo::createUnbufferizePass()";
 }
 
+def TileLinalgCopyPass : Pass<"tile-linalg-copy"> {
+  let summary = "Tiles linalg copy ops by 1.";
+  let constructor = "hlo::createTileLinalgCopyPass(1)";
+  let dependentDialects = ["linalg::LinalgDialect", "scf::SCFDialect"];
+}
+
+def LinalgCopyToMemrefPass : Pass<"linalg-copy-to-memref", "func::FuncOp"> {
+  let summary = "Rewrites a linalg.copy on memrefs to a memref.copy.";
+  let constructor = "hlo::createLinalgCopyToMemrefPass()";
+  let dependentDialects = ["linalg::LinalgDialect"];
+}
+
 def UnrollLoopsPass : Pass<"unroll-loops"> {
   let summary = "Unrolls scf.for loops with small static iteration counts.";
   let constructor = "hlo::createUnrollLoopsPass()";
diff --git a/xla/service/cpu/cpu_compiler.cc b/xla/service/cpu/cpu_compiler.cc
index d985ca2867..7112ec53a4 100644
--- a/xla/service/cpu/cpu_compiler.cc
+++ b/xla/service/cpu/cpu_compiler.cc
@@ -261,6 +261,11 @@ xla::cpu::HloXlaRuntimePipelineOptions GetHloXlaRuntimePipelineOptions(
   xla::cpu::HloXlaRuntimePipelineOptions options;
   options.enable_tiling_and_fusion =
       xla::GetDebugOptionsFromFlags().xla_cpu_enable_mlir_tiling_and_fusion();
+  options.enable_concat_optimization =
+      xla::GetDebugOptionsFromFlags().xla_cpu_enable_concat_optimization();
+  if (options.enable_concat_optimization) {
+    options.sparse_bufferization = false;
+  }
   if (xla::GetDebugOptionsFromFlags().xla_cpu_enable_custom_matmul_tiling()) {
     options.matmul_tile_sizes = {
         xla::GetDebugOptionsFromFlags().xla_cpu_matmul_tiling_m_dim(),
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.cc b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
index fc2c13fb1c..775f4479e4 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.cc
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
@@ -42,6 +42,7 @@ limitations under the License.
 #include "mlir/Dialect/SparseTensor/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
 #include "mlir/Dialect/SparseTensor/Transforms/Passes.h"  // from @llvm-project
 #include "mlir/Dialect/Tensor/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
+#include "mlir/Dialect/Tensor/Transforms/Passes.h" // from @llvm-project
 #include "mlir/Dialect/Vector/Transforms/BufferizableOpInterfaceImpl.h"  // from @llvm-project
 #include "mlir/Pass/PassManager.h"  // from @llvm-project
 #include "mlir/Transforms/Passes.h"  // from @llvm-project
@@ -199,7 +200,15 @@ static Status CreateHloXlaPipeline(
 
   // Tile tHLO ops to 1.
   if (!options.enable_tiling_and_fusion) {
-    pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
+    if (options.enable_concat_optimization) {
+      // Instead of tiling by one the thlo.concat (thus generating an
+      // elementwise memory copy), replace it with a tensor.concat.
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::thlo::createLegalizeConcatPass());
+      pm.addPass(mlir::tensor::createDecomposeTensorConcatPass());
+    }
+    else {
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
+    }
   }
 
   // Lower shape dialect to standard to enable linalg canonicalizations (e.g.
@@ -260,7 +269,13 @@ static Status CreateHloXlaPipeline(
     AddSparsificationPasses(pm, options.experimental_deallocation,
                             options.xla_cpu_sparse_cuda_threads);
   } else {
-    pm.addPass(mlir::hlo::createOneShotBufferizePass());
+    pm.addPass(mlir::hlo::createOneShotBufferizePass(options.enable_concat_optimization));
+    if (options.enable_concat_optimization) {
+      // Tiling the linalg copy by 1 makes the linalg.copy work on contiguous
+      // data, which significantly improves copy performance.
+      pm.addPass(mlir::hlo::createTileLinalgCopyPass(1));
+      pm.addNestedPass<mlir::func::FuncOp>(mlir::hlo::createLinalgCopyToMemrefPass());
+    }
   }
   pm.addNestedPass<mlir::func::FuncOp>(createRewriteReallocToAllocPass());
 
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.h b/xla/service/cpu/hlo_xla_runtime_pipeline.h
index b77436c165..64a0160f6d 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.h
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.h
@@ -32,6 +32,7 @@ namespace cpu {
 struct HloXlaRuntimePipelineOptions {
   bool enable_tiling_and_fusion = false;
   bool enable_fusion_outlining = true;
+  bool enable_concat_optimization = true;
   bool remove_copies_to_outparams = true;
   bool sparse_bufferization = true;
   bool experimental_deallocation = false;
diff --git a/xla/xla.proto b/xla/xla.proto
index 7e01954d0c..0e5f8b4276 100644
--- a/xla/xla.proto
+++ b/xla/xla.proto
@@ -531,6 +531,7 @@ message DebugOptions {
   // is enabled, the pipeline will use tiling, fusion, peeling, vectorization
   // instead.
   bool xla_cpu_enable_mlir_tiling_and_fusion = 184;
+  bool xla_cpu_enable_concat_optimization = 258;
 
   // XLA:CPU-Next tiling parameters for matmul.
   bool xla_cpu_enable_custom_matmul_tiling = 195;
