diff --git a/xla/debug_options_flags.cc b/xla/debug_options_flags.cc
index 9f595d74d8..d070de671d 100644
--- a/xla/debug_options_flags.cc
+++ b/xla/debug_options_flags.cc
@@ -162,6 +162,9 @@ DebugOptions DefaultDebugOptionsIgnoringFlags() {

   opts.set_xla_cpu_enable_mlir_tiling_and_fusion(true);
   opts.set_xla_cpu_enable_concat_optimization(false);
+  opts.set_xla_cpu_enable_concat_removal(false);
+  opts.set_xla_cpu_enable_concat_canonicalization(false);
+  opts.set_xla_cpu_use_matmul_library(false);
   opts.set_xla_cpu_enable_custom_matmul_tiling(false);
   opts.set_xla_cpu_matmul_tiling_m_dim(8);
   opts.set_xla_cpu_matmul_tiling_n_dim(8);
@@ -1073,6 +1076,28 @@ void MakeDebugOptionsFlags(std::vector<tsl::Flag>* flag_list,
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_concat_optimization),
       debug_options->xla_cpu_enable_concat_optimization(),
       "Enable concat optimization."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_enable_concat_removal",
+      bool_setter_for(&DebugOptions::set_xla_cpu_enable_concat_removal),
+      debug_options->xla_cpu_enable_concat_removal(),
+      "Enable concat removal."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_enable_concat_canonicalization",
+      bool_setter_for(&DebugOptions::set_xla_cpu_enable_concat_canonicalization),
+      debug_options->xla_cpu_enable_concat_canonicalization(),
+      "Enable concat canonicalization."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_use_matmul_library",
+      bool_setter_for(&DebugOptions::set_xla_cpu_use_matmul_library),
+      debug_options->xla_cpu_use_matmul_library(),
+      "Replace mhlo.dot and mhlo.dot_general with library calls to "
+      "an external library."));
+  flag_list->push_back(tsl::Flag(
+      "xla_cpu_enable_output_tensor_reuse",
+      bool_setter_for(&DebugOptions::set_xla_cpu_enable_output_tensor_reuse),
+      debug_options->xla_cpu_enable_output_tensor_reuse(),
+      "Replace the output tensor of gml.st_fusion ops with the "
+      "input tensor when shape matches and it is legal to do so."));
   flag_list->push_back(tsl::Flag(
       "xla_cpu_enable_mlir_fusion_outlining",
       bool_setter_for(&DebugOptions::set_xla_cpu_enable_mlir_fusion_outlining),
diff --git a/xla/mlir_hlo/deallocation/transforms/buffer_reuse.cc b/xla/mlir_hlo/deallocation/transforms/buffer_reuse.cc
index 3c9d735e6b..7543ccb788 100644
--- a/xla/mlir_hlo/deallocation/transforms/buffer_reuse.cc
+++ b/xla/mlir_hlo/deallocation/transforms/buffer_reuse.cc
@@ -513,6 +513,12 @@ void promoteBuffers(Block& block) {

 struct BufferReusePass : public impl::BufferReusePassBase<BufferReusePass> {
   void runOnOperation() override {
+    func::FuncOp func = dyn_cast<func::FuncOp>(getOperation());
+    assert(func);
+    // Do not run the pass on bodiless functions
+    if (func.isDeclaration())
+      return;
+
     bool result;
     auto& block = getOperation().getBody().front();
     // Copy elimination requires small live-ranges to work well. We only extend
diff --git a/xla/mlir_hlo/deallocation/transforms/deallocate.cc b/xla/mlir_hlo/deallocation/transforms/deallocate.cc
index 90a1ed051f..10079818d1 100644
--- a/xla/mlir_hlo/deallocation/transforms/deallocate.cc
+++ b/xla/mlir_hlo/deallocation/transforms/deallocate.cc
@@ -127,6 +127,9 @@ Value Deallocator::findOwnershipIndicator(Value v) {
 LogicalResult Deallocator::transformModuleOp(ModuleOp op) {
   LogicalResult result = success();
   op.walk([&](func::FuncOp funcOp) {
+    if (funcOp.isDeclaration())
+      return WalkResult::advance();
+
     if (failed(transformFuncOp(funcOp))) {
       result = failure();
       return WalkResult::interrupt();
@@ -212,6 +215,15 @@ FailureOr<TransformResult> Deallocator::transformBlock(Block& block,

   TransformResult blockResult;
   for (auto& op : llvm::make_early_inc_range(block.without_terminator())) {
+    if (auto callOp = llvm::dyn_cast<func::CallOp>(op)) {
+      auto callee = llvm::cast<func::FuncOp>(
+        callOp->getParentOfType<ModuleOp>().lookupSymbol(callOp.getCallee()));
+      // If the op is a call op to a bodiless function, skip it, since we will
+      // not be able to transform the function body (since it is not present).
+      if (callee.isDeclaration())
+        continue;
+    }
+
     auto opResult = transformOp(&op, ownedMemrefs);
     if (failed(opResult)) return failure();
     // Remove released memrefs.
diff --git a/xla/mlir_hlo/deallocation/transforms/split_alloc_tensors.cc b/xla/mlir_hlo/deallocation/transforms/split_alloc_tensors.cc
index 0d0bf7a6a7..ed73303b9b 100644
--- a/xla/mlir_hlo/deallocation/transforms/split_alloc_tensors.cc
+++ b/xla/mlir_hlo/deallocation/transforms/split_alloc_tensors.cc
@@ -55,7 +55,12 @@ void splitAllocTensors(Block& block) {
 struct SplitAllocTensorsPass
     : public impl::SplitAllocTensorsPassBase<SplitAllocTensorsPass> {
   void runOnOperation() override {
-    splitAllocTensors(getOperation().getBody().front());
+    func::FuncOp func = dyn_cast<func::FuncOp>(getOperation());
+    assert(func);
+
+    // Do not run the pass on bodiless functions
+    if (!func.isDeclaration())
+      splitAllocTensors(getOperation().getBody().front());
   }
 };

diff --git a/xla/mlir_hlo/gml_st/interfaces/bufferizable_op_interface_impl.cc b/xla/mlir_hlo/gml_st/interfaces/bufferizable_op_interface_impl.cc
index 398d78c9cd..4c424aad79 100644
--- a/xla/mlir_hlo/gml_st/interfaces/bufferizable_op_interface_impl.cc
+++ b/xla/mlir_hlo/gml_st/interfaces/bufferizable_op_interface_impl.cc
@@ -29,6 +29,10 @@ namespace mlir {
 namespace gml_st {
 namespace {

+// TODO: Use the hasLabel and kElementwiseLabel from the original place rather than copy/pasting it.
+bool hasLabel(Operation *op, StringRef name) { return op->hasAttr(name); }
+constexpr llvm::StringRef kElementwiseLabel = "__elementwise_label__";
+
 using mlir::bufferization::AliasingOpOperandList;
 using mlir::bufferization::AliasingValueList;
 using mlir::bufferization::AnalysisState;
@@ -77,6 +81,16 @@ struct FusionOpBufferizationInterface
     return true;
   }

+  bool isNotConflicting(Operation *op, OpOperand *uRead,
+                        OpOperand *uConflictingWrite,
+                        const AnalysisState &state) const {
+    // There are no conflicts in element-wise operations.
+    // This idea is already implemented upstream, see:
+    // https://github.com/llvm/llvm-project/commit/5468340
+    FusionOp fusionOp = cast<FusionOp>(op);
+    return hasLabel(fusionOp, kElementwiseLabel);
+  }
+
   LogicalResult bufferize(Operation *op, RewriterBase &rewriter,
                           const BufferizationOptions &options) const {
     // Take a guard before anything else.
diff --git a/xla/mlir_hlo/gml_st/transforms/cpu_tiling/cpu_tiling_pipeline.cc b/xla/mlir_hlo/gml_st/transforms/cpu_tiling/cpu_tiling_pipeline.cc
index 5e2649621f..09f1c0e8a7 100644
--- a/xla/mlir_hlo/gml_st/transforms/cpu_tiling/cpu_tiling_pipeline.cc
+++ b/xla/mlir_hlo/gml_st/transforms/cpu_tiling/cpu_tiling_pipeline.cc
@@ -43,6 +43,7 @@ GmlStCPUTilingOptions getDefaultCPUPipelineOptions(StringRef cpuName,
   opts.statsDetailLevel = statsDetailLevel;
   opts.fuseDegenerateReshapes = false;
   opts.inlineFusionClusters = true;
+  opts.outputTensorReuse = false;
   return opts;
 }

@@ -97,6 +98,9 @@ void addCPUTilingPipeline(OpPassManager& pm,
   pm.addNestedPass<FuncOp>(createScalarizationPass());
   pm.addNestedPass<FuncOp>(createComposeExtractInsertSlicePass());

+  if (options.outputTensorReuse)
+    pm.addPass(createReuseOutputTensorPass());
+
   pm.addPass(createCanonicalizerPass());

   // Remove transformed labels after tiling all ops.
diff --git a/xla/mlir_hlo/gml_st/transforms/passes.h b/xla/mlir_hlo/gml_st/transforms/passes.h
index 12b02debc7..ef3c2aa671 100644
--- a/xla/mlir_hlo/gml_st/transforms/passes.h
+++ b/xla/mlir_hlo/gml_st/transforms/passes.h
@@ -111,6 +111,9 @@ createFusionPlanningForCpuPass(int64_t vectorSize = 8);
 /// Pass to outline fusion regions into functions.
 std::unique_ptr<OperationPass<mlir::ModuleOp>> createFusionOutliningPass();

+/// Pass to reuse output tensors in gml.st_fusion ops.
+std::unique_ptr<OperationPass<mlir::ModuleOp>> createReuseOutputTensorPass();
+
 /// Pass to inline fusion clusters.
 std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>>
 createInlineFusionClustersPass();
@@ -156,6 +159,7 @@ struct GmlStCPUTilingOptions
     this->statsDetailLevel = opts.statsDetailLevel;
     this->cpuName = opts.cpuName;
     this->inlineFusionClusters = opts.inlineFusionClusters;
+    this->outputTensorReuse = opts.outputTensorReuse;
   }

   Option<int64_t> vectorSize{*this, "vector-size",
@@ -228,6 +232,11 @@ struct GmlStCPUTilingOptions
       *this, "inline-fusion-clusters",
       llvm::cl::desc("Inline fusion clusters at the end of the pipeline."),
       llvm::cl::init(true)};
+
+  Option<bool> outputTensorReuse{
+      *this, "output-tensor-reuse",
+      llvm::cl::desc("Reuse output tensors in gml.st_fusion ops."),
+      llvm::cl::init(false)};
 };

 // Returns default "optimized" tiling parameters.
diff --git a/xla/mlir_hlo/gml_st/transforms/passes.td b/xla/mlir_hlo/gml_st/transforms/passes.td
index ce462a3adc..b84492bdca 100644
--- a/xla/mlir_hlo/gml_st/transforms/passes.td
+++ b/xla/mlir_hlo/gml_st/transforms/passes.td
@@ -242,6 +242,12 @@ def FusionOutliningPass : Pass<"gml-fusion-outlining", "mlir::ModuleOp"> {
   let constructor = "createFusionOutliningPass()";
 }

+def ReuseOutputTensorPass : Pass<"gml-st-reuse-output-tensor", "mlir::ModuleOp"> {
+  let summary = "Replace the output tensor of gml.st_fusion ops with the"
+                " input tensor when shape matches and it is legal to do so.";
+  let constructor = "createReuseOutputTensorPass()";
+}
+
 def InlineFusionClustersPass :
     Pass<"gml-st-inline-fusion-clusters", "mlir::func::FuncOp"> {
   let summary = "Replaces all gml_st.fusion op with ops from the region.";
diff --git a/xla/mlir_hlo/transforms/passes.h b/xla/mlir_hlo/transforms/passes.h
index 4d89118b55..ed124ed0eb 100644
--- a/xla/mlir_hlo/transforms/passes.h
+++ b/xla/mlir_hlo/transforms/passes.h
@@ -108,6 +108,8 @@ void registerTestHloTransformDialectInterpreterPass();
 namespace hlo {
 std::unique_ptr<OperationPass<ModuleOp>> createOneShotBufferizePass(bool useLinalgCopyFn);

+std::unique_ptr<Pass> createDotToFunctionCallPass();
+
 std::unique_ptr<Pass> createTileLinalgCopyPass(int tileSize);

 std::unique_ptr<mlir::OperationPass<mlir::func::FuncOp>> createLinalgCopyToMemrefPass();
diff --git a/xla/mlir_hlo/transforms/passes.td b/xla/mlir_hlo/transforms/passes.td
index 9909505055..4649431ce3 100644
--- a/xla/mlir_hlo/transforms/passes.td
+++ b/xla/mlir_hlo/transforms/passes.td
@@ -158,6 +158,11 @@ def UnbufferizePass : Pass<"unbufferize", "mlir::func::FuncOp"> {
   let constructor = "hlo::createUnbufferizePass()";
 }

+def DotToFunctionCallPass : Pass<"dot-to-function-call"> {
+  let summary = "Lower MHLO dot and dot_general ops to a function call";
+  let constructor = "hlo::createDotToFunctionCallPass()";
+}
+
 def TileLinalgCopyPass : Pass<"tile-linalg-copy"> {
   let summary = "Tiles linalg copy ops by 1.";
   let constructor = "hlo::createTileLinalgCopyPass(1)";
diff --git a/xla/service/cpu/cpu_compiler.cc b/xla/service/cpu/cpu_compiler.cc
index 7112ec53a4..e27b3697f0 100644
--- a/xla/service/cpu/cpu_compiler.cc
+++ b/xla/service/cpu/cpu_compiler.cc
@@ -263,6 +263,15 @@ xla::cpu::HloXlaRuntimePipelineOptions GetHloXlaRuntimePipelineOptions(
       xla::GetDebugOptionsFromFlags().xla_cpu_enable_mlir_tiling_and_fusion();
   options.enable_concat_optimization =
       xla::GetDebugOptionsFromFlags().xla_cpu_enable_concat_optimization();
+  options.use_matmul_library =
+      xla::GetDebugOptionsFromFlags().xla_cpu_use_matmul_library();
+  options.enable_output_tensor_reuse =
+      xla::GetDebugOptionsFromFlags().xla_cpu_enable_output_tensor_reuse();
+  options.enable_concat_removal =
+      xla::GetDebugOptionsFromFlags().xla_cpu_enable_concat_removal();
+  options.enable_concat_canonicalization =
+      xla::GetDebugOptionsFromFlags().xla_cpu_enable_concat_canonicalization();
+
   if (options.enable_concat_optimization) {
     options.sparse_bufferization = false;
   }
@@ -275,6 +284,12 @@ xla::cpu::HloXlaRuntimePipelineOptions GetHloXlaRuntimePipelineOptions(
   options.experimental_deallocation =
       xla::GetDebugOptionsFromFlags()
           .xla_cpu_enable_experimental_deallocation();
+  if (options.use_matmul_library) {
+    // Experimental deallocation segfaults if we
+    // enable xla_cpu_use_matmul_library.
+    options.experimental_deallocation = false;
+  }
+
   options.enable_avx2 = [&] {
     // Derive whether this is an x86 CPU with AVX2 enabled.
     if (!target_triple.isX86()) return false;
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.cc b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
index 5e062c0755..eabc265b5b 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.cc
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.cc
@@ -142,6 +142,8 @@ static Status CheckHloXlaRuntimePipelineOptionsLegality(HloXlaRuntimePipelineOpt
     if (!options.enable_tiling_and_fusion || !options.enable_fusion_outlining)
       return absl::InternalError("profiling was requested with XLA_PROFILE_FUSION_CLUSTER, but tiling_and_fusion and/or fusion_outlining are disabled");
   }
+  if (!options.enable_tiling_and_fusion && options.enable_output_tensor_reuse)
+    return absl::InternalError("cannot apply output_tensor_reuse with tiling_and_fusion disabled");
   return absl::OkStatus();
 }

@@ -188,6 +190,11 @@ static Status CreateHloXlaPipeline(
         mlir::mhlo::createSparseRewritingPass());
   }

+  if (options.use_matmul_library) {
+    // Lower dot operations to function library calls
+    pm.addPass(mlir::hlo::createDotToFunctionCallPass());
+  }
+
   // Transform HLO operations to Linalg.
   pm.addNestedPass<mlir::func::FuncOp>(
       mlir::mhlo::createLegalizeControlFlowPass());
@@ -213,16 +220,25 @@ static Status CreateHloXlaPipeline(
   pm.addPass(mlir::mhlo::createConvertToSignlessPass());

   // Tile tHLO ops to 1.
-  if (!options.enable_tiling_and_fusion) {
-    if (options.enable_concat_optimization) {
-      // Instead of tiling by one the thlo.concat (thus generating an
-      // elementwise memory copy), replace it with a tensor.concat.
-      pm.addNestedPass<mlir::func::FuncOp>(mlir::thlo::createLegalizeConcatPass());
-      pm.addPass(mlir::tensor::createDecomposeTensorConcatPass());
-    }
-    else {
-      pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
-    }
+  if (options.enable_concat_optimization || options.enable_concat_removal || options.enable_concat_canonicalization) {
+    // Instead of tiling by one the thlo.concat (thus generating an
+    // elementwise memory copy), replace it with a tensor.concat.
+    pm.addNestedPass<mlir::func::FuncOp>(mlir::thlo::createLegalizeConcatPass());
+
+    // If more than one optimization is enabled, then, in this order, we:
+    // 1. Try to remove (or simplify) concat.
+    // 2. Try to canonicalize tensor_insert slices into the concat.
+    // 3. Optimize concat lowering.
+    if (options.enable_concat_removal)
+      pm.addPass(mlir::tensor::createConcatRemovalPass());
+    //if (options.enable_concat_canonicalization)
+    //  pm.addPass(mlir::tensor::createSimplifyTensorConcatPass());
+
+    // Lower concat into a tensor.empty plus a set of insert_slice ops.
+    pm.addPass(mlir::tensor::createDecomposeTensorConcatPass());
+  }
+  else {
+    pm.addNestedPass<mlir::func::FuncOp>(mlir::gml_st::createTileByOnePass());
   }

   // Lower shape dialect to standard to enable linalg canonicalizations (e.g.
@@ -244,6 +260,8 @@ static Status CreateHloXlaPipeline(
         mlir::gml_st::getDefaultCPUPipelineOptions(options.cpu_name);
     opts.matmulTileSizes = options.matmul_tile_sizes;
     opts.inlineFusionClusters = false;
+    if (options.enable_output_tensor_reuse)
+      opts.outputTensorReuse = true;
     mlir::gml_st::addCPUTilingPipeline(pm, opts);
   } else {
     pm.addNestedPass<mlir::func::FuncOp>(
diff --git a/xla/service/cpu/hlo_xla_runtime_pipeline.h b/xla/service/cpu/hlo_xla_runtime_pipeline.h
index 64a0160f6d..e6f6db615b 100644
--- a/xla/service/cpu/hlo_xla_runtime_pipeline.h
+++ b/xla/service/cpu/hlo_xla_runtime_pipeline.h
@@ -33,6 +33,10 @@ struct HloXlaRuntimePipelineOptions {
   bool enable_tiling_and_fusion = false;
   bool enable_fusion_outlining = true;
   bool enable_concat_optimization = true;
+  bool use_matmul_library = false;
+  bool enable_output_tensor_reuse = false;
+  bool enable_concat_removal = false;
+  bool enable_concat_canonicalization = false;
   bool remove_copies_to_outparams = true;
   bool sparse_bufferization = true;
   bool experimental_deallocation = false;
diff --git a/xla/xla.proto b/xla/xla.proto
index 0e5f8b4276..f3beb775aa 100644
--- a/xla/xla.proto
+++ b/xla/xla.proto
@@ -532,6 +532,10 @@ message DebugOptions {
   // instead.
   bool xla_cpu_enable_mlir_tiling_and_fusion = 184;
   bool xla_cpu_enable_concat_optimization = 258;
+  bool xla_cpu_use_matmul_library = 259;
+  bool xla_cpu_enable_output_tensor_reuse = 260;
+  bool xla_cpu_enable_concat_removal = 261;
+  bool xla_cpu_enable_concat_canonicalization = 262;

   // XLA:CPU-Next tiling parameters for matmul.
   bool xla_cpu_enable_custom_matmul_tiling = 195;
