diff --git a/third_party/xla/xla/service/cpu/ir_emitter.cc b/third_party/xla/xla/service/cpu/ir_emitter.cc
index 7013a45f..c4974226 100644
--- a/third_party/xla/xla/service/cpu/ir_emitter.cc
+++ b/third_party/xla/xla/service/cpu/ir_emitter.cc
@@ -46,6 +46,9 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsX86.h"
 #include "llvm/IR/LLVMContext.h"
 #include "llvm/IR/Value.h"
+#include "tsl/lib/math/math_util.h"
+#include "tsl/platform/errors.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_casting_utils.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_instructions.h"
@@ -63,6 +66,8 @@ limitations under the License.
 #include "xla/service/cpu/ir_emission_utils.h"
 #include "xla/service/cpu/ir_function.h"
 #include "xla/service/cpu/parallel_loop_emitter.h"
+#include <xnnpack_ops.h>
+#include <xnnpack_ops_rewriter.h>
 #include "xla/service/elemental_ir_emitter.h"
 #include "xla/service/llvm_ir/buffer_assignment_util.h"
 #include "xla/service/llvm_ir/dynamic_update_slice_util.h"
@@ -77,9 +82,6 @@ limitations under the License.
 #include "xla/util.h"
 #include "xla/window_util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/lib/math/math_util.h"
-#include "tsl/platform/errors.h"
-#include "tsl/platform/logging.h"
 
 #if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)
 #include "xla/service/cpu/onednn_memory_util.h"
@@ -2471,6 +2473,39 @@ Status IrEmitter::HandleOneDnnMatMul(HloInstruction* custom_call) {
 }
 #endif  // INTEL_MKL && ENABLE_ONEDNN_V3
 
+Status IrEmitter::HandleXnnPackSoftMax(HloInstruction* hlo) {
+  const HloInstruction* input = hlo->operand(0);
+  Shape shape = input->shape();
+
+  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(hlo));
+  TF_RET_CHECK(input->shape().element_type() == F32);
+  TF_RET_CHECK(shape.rank() >= 2);
+
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice input_values_slice,
+                      assignment_.GetUniqueSlice(hlo->operand(0), {}));
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice out_values_slice,
+                      assignment_.GetUniqueSlice(hlo, {}));
+
+  llvm::Value* values_ptr = EmitBufferPointer(input_values_slice, shape);
+  llvm::Value* out_values_ptr = EmitBufferPointer(out_values_slice, shape);
+
+  // Flatten the batches into a single dimension.
+  int channels = shape.dimensions(shape.rank() - 1);
+  int batch_size = 1;
+  for (int i = 0; i < shape.rank() - 1; i++)
+    batch_size = batch_size * shape.dimensions(i);
+
+  EmitCallToFunc(runtime::kXnnPackSoftMaxNDSymbolName,
+                 {/*run_options=*/GetExecutableRunOptionsArgument(),
+                  /*input*/ values_ptr,
+                  /*output*/ out_values_ptr,
+                  /*batch_size*/ b_.getInt64(batch_size),
+                  /*channels*/ b_.getInt64(channels)},
+                 b_.getVoidTy());
+
+  return OkStatus();
+}
+
 Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {
   if (custom_call->custom_call_target() == "PadToStatic") {
     return HandlePadToStatic(custom_call);
@@ -2481,6 +2516,9 @@ Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {
   if (custom_call->custom_call_target() == "TopK") {
     return HandleTopK(custom_call);
   }
+  if (custom_call->custom_call_target() == kCustomCallXnnPackSoftMax) {
+    return HandleXnnPackSoftMax(custom_call);
+  }
 #if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)
   if (custom_call->custom_call_target() == "__onednn$matmul") {
     return HandleOneDnnMatMul(custom_call);

