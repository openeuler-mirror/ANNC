diff --git a/xla/service/cpu/ir_emitter.cc b/xla/service/cpu/ir_emitter.cc
index 7013a45f1e..0043e144f6 100644
--- a/xla/service/cpu/ir_emitter.cc
+++ b/xla/service/cpu/ir_emitter.cc
@@ -46,6 +46,9 @@ limitations under the License.
 #include "llvm/IR/IntrinsicsX86.h"
 #include "llvm/IR/LLVMContext.h"
 #include "llvm/IR/Value.h"
+#include "tsl/lib/math/math_util.h"
+#include "tsl/platform/errors.h"
+#include "tsl/platform/logging.h"
 #include "xla/hlo/ir/hlo_casting_utils.h"
 #include "xla/hlo/ir/hlo_instruction.h"
 #include "xla/hlo/ir/hlo_instructions.h"
@@ -63,6 +66,8 @@ limitations under the License.
 #include "xla/service/cpu/ir_emission_utils.h"
 #include "xla/service/cpu/ir_function.h"
 #include "xla/service/cpu/parallel_loop_emitter.h"
+#include <xnnpack_ops.h>
+#include <xnnpack_ops_rewriter.h>
 #include "xla/service/elemental_ir_emitter.h"
 #include "xla/service/llvm_ir/buffer_assignment_util.h"
 #include "xla/service/llvm_ir/dynamic_update_slice_util.h"
@@ -77,9 +82,6 @@ limitations under the License.
 #include "xla/util.h"
 #include "xla/window_util.h"
 #include "xla/xla_data.pb.h"
-#include "tsl/lib/math/math_util.h"
-#include "tsl/platform/errors.h"
-#include "tsl/platform/logging.h"
 
 #if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)
 #include "xla/service/cpu/onednn_memory_util.h"
@@ -2471,6 +2473,155 @@ Status IrEmitter::HandleOneDnnMatMul(HloInstruction* custom_call) {
 }
 #endif  // INTEL_MKL && ENABLE_ONEDNN_V3
 
+Status IrEmitter::HandleXnnPackSoftMax(HloInstruction* hlo) {
+  const HloInstruction* input = hlo->operand(0);
+  Shape shape = input->shape();
+
+  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(hlo));
+  TF_RET_CHECK(input->shape().element_type() == F32);
+  TF_RET_CHECK(shape.rank() >= 2);
+
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice input_values_slice,
+                      assignment_.GetUniqueSlice(hlo->operand(0), {}));
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice out_values_slice,
+                      assignment_.GetUniqueSlice(hlo, {}));
+
+  llvm::Value* values_ptr = EmitBufferPointer(input_values_slice, shape);
+  llvm::Value* out_values_ptr = EmitBufferPointer(out_values_slice, shape);
+
+  // Flatten the batches into a single dimension.
+  int channels = shape.dimensions(shape.rank() - 1);
+  int batch_size = 1;
+  for (int i = 0; i < shape.rank() - 1; i++)
+    batch_size = batch_size * shape.dimensions(i);
+
+  EmitCallToFunc(runtime::kXnnPackSoftMaxNDSymbolName,
+                 {/*run_options=*/GetExecutableRunOptionsArgument(),
+                  /*input*/ values_ptr,
+                  /*output*/ out_values_ptr,
+                  /*batch_size*/ b_.getInt64(batch_size),
+                  /*channels*/ b_.getInt64(channels)},
+                 b_.getVoidTy());
+
+  return OkStatus();
+}
+
+Status IrEmitter::HandleKernelSelector(HloInstruction* custom_call) {
+  OpMetadata metadata = custom_call->metadata();
+
+  bool isGEMV = (metadata.op_type() == runtime::kKernelSelectorOperationGEMV);
+  bool isGEMM = (metadata.op_type() == runtime::kKernelSelectorOperationGEMM);
+  bool isBATCHMATMUL3D =
+      (metadata.op_type() == runtime::kKernelSelectorOperationBATCH3D);
+  bool isBATCHMATMUL4D =
+      (metadata.op_type() == runtime::kKernelSelectorOperationBATCH4D);
+  bool isBATCHMATMUL = isBATCHMATMUL3D | isBATCHMATMUL4D;
+
+  int operand = 0;
+  std::vector<llvm::Value*> arguments;
+
+  //  |               arguments               |
+  //  |  gemm  |  batch3d |  batch4d | gemv   |
+  //  -----------------------------------------
+  //  |  trA   |  trA     |  trA     |  trA   |
+  //  |  trB   |  trB     |  trB     |        |
+  //  |  A     |  A       |  A       |  A     |
+  //  |  B     |  B       |  B       |  X     |
+  //  |        |          |  Q       |        |
+  //  |        |  P       |  P       |        |
+  //  |  M     |  M       |  M       |  M     |
+  //  |  N     |  N       |  N       |  N     |
+  //  |  K     |  K       |  K       |        |
+  //  |  alpha |          |          |  alpha |
+  //  |  beta  |          |          |  beta  |
+
+  // trA
+  HloInstruction const* trA = custom_call->operand(operand++);
+  bool tranA = trA->literal().Get<bool>({});
+  arguments.push_back(b_.getInt1(tranA));
+
+  if (isGEMM || isBATCHMATMUL) {
+    // trB
+    HloInstruction const* trB = custom_call->operand(operand++);
+    bool tranB = trB->literal().Get<bool>({});
+    arguments.push_back(b_.getInt1(tranB));
+  }
+
+  // A
+  HloInstruction const* A = custom_call->operand(operand++);
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice a_slice,
+                      assignment_.GetUniqueSlice(A, {}));
+  llvm::Value* A_ptr = EmitBufferPointer(a_slice, A->shape());
+  arguments.push_back(A_ptr);
+
+  // B (or X in GEMV)
+  HloInstruction const* B = custom_call->operand(operand++);
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice b_slice,
+                      assignment_.GetUniqueSlice(B, {}));
+  llvm::Value* B_ptr = EmitBufferPointer(b_slice, B->shape());
+  arguments.push_back(B_ptr);
+
+  if (isBATCHMATMUL) {
+    // Q
+    if (isBATCHMATMUL4D) {
+      HloInstruction const* Q = custom_call->operand(operand++);
+      int q = Q->literal().Get<int>({});
+      arguments.push_back(b_.getInt32(q));
+    }
+
+    // P
+    HloInstruction const* P = custom_call->operand(operand++);
+    int p = P->literal().Get<int>({});
+    arguments.push_back(b_.getInt32(p));
+  }
+
+  // M
+  HloInstruction const* M = custom_call->operand(operand++);
+  int m = M->literal().Get<int>({});
+  arguments.push_back(b_.getInt32(m));
+
+  // N
+  HloInstruction const* N = custom_call->operand(operand++);
+  int n = N->literal().Get<int>({});
+  arguments.push_back(b_.getInt32(n));
+
+  if (isGEMM || isBATCHMATMUL) {
+    // K
+    HloInstruction const* K = custom_call->operand(operand++);
+    int k = K->literal().Get<int>({});
+    arguments.push_back(b_.getInt32(k));
+  }
+
+  float beta = 0.0;
+  if (isGEMM || isGEMV) {
+    // Alpha
+    HloInstruction const* Alpha = custom_call->operand(operand++);
+    float alpha = Alpha->literal().Get<float>({});
+    llvm::Constant* alphaConst = llvm::ConstantFP::get(b_.getFloatTy(), alpha);
+    arguments.push_back(alphaConst);
+
+    // Beta
+    HloInstruction const* Beta = custom_call->operand(operand++);
+    beta = Beta->literal().Get<float>({});
+    llvm::Constant* betaConst = llvm::ConstantFP::get(b_.getFloatTy(), beta);
+    arguments.push_back(betaConst);
+  }
+
+  // C (or Y in GEMV)
+  HloInstruction const* C = custom_call;
+
+  TF_ASSIGN_OR_RETURN(const BufferAllocation::Slice c_slice,
+                      assignment_.GetUniqueSlice(C, {}));
+  llvm::Value* C_ptr = EmitBufferPointer(c_slice, C->shape());
+  arguments.push_back(C_ptr);
+
+  TF_RETURN_IF_ERROR(EmitTargetAddressForOp(custom_call));
+
+  EmitCallToFunc(metadata.op_name(), arguments, b_.getVoidTy());
+
+  return OkStatus();
+}
+
 Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {
   if (custom_call->custom_call_target() == "PadToStatic") {
     return HandlePadToStatic(custom_call);
@@ -2478,9 +2629,15 @@ Status IrEmitter::HandleCustomCall(HloInstruction* custom_call) {
   if (custom_call->custom_call_target() == "SliceToDynamic") {
     return HandleSliceToDynamic(custom_call);
   }
+  if (custom_call->custom_call_target() == "KernelSelector") {
+    return HandleKernelSelector(custom_call);
+  }
   if (custom_call->custom_call_target() == "TopK") {
     return HandleTopK(custom_call);
   }
+  if (custom_call->custom_call_target() == kCustomCallXnnPackSoftMax) {
+    return HandleXnnPackSoftMax(custom_call);
+  }
 #if defined(INTEL_MKL) && defined(ENABLE_ONEDNN_V3)
   if (custom_call->custom_call_target() == "__onednn$matmul") {
     return HandleOneDnnMatMul(custom_call);
